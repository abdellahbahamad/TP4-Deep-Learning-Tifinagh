{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76c534d0-b80d-4c4a-9bbb-a2bfe17af26e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train directory: C:\\Users\\Dell\\Desktop\\Tifinagh\\train_data\n",
      "Test directory: C:\\Users\\Dell\\Desktop\\Tifinagh\\test_data\n",
      "Current working directory: C:\\Users\\Dell\\Desktop\\Tifinagh\n",
      "Loaded 28182 training samples with 33 unique classes.\n",
      "Loaded 28182 test samples with 33 unique classes.\n",
      "Train: 21136 samples, Validation: 7046 samples, Test: 28182 samples\n",
      "Epoch 0, Train Loss: 2.4338, Val Loss: 1.9104, Train Acc: 0.4233, Val Acc: 0.4112\n",
      "Epoch 10, Train Loss: 0.4698, Val Loss: 0.5341, Train Acc: 0.8716, Val Acc: 0.8274\n",
      "Epoch 20, Train Loss: 0.2331, Val Loss: 0.3779, Train Acc: 0.9399, Val Acc: 0.8789\n",
      "Epoch 30, Train Loss: 0.1476, Val Loss: 0.3480, Train Acc: 0.9560, Val Acc: 0.8909\n",
      "Epoch 40, Train Loss: 0.1147, Val Loss: 0.3255, Train Acc: 0.9700, Val Acc: 0.9018\n",
      "Epoch 50, Train Loss: 0.0959, Val Loss: 0.2889, Train Acc: 0.9823, Val Acc: 0.9127\n",
      "Epoch 60, Train Loss: 0.0801, Val Loss: 0.2907, Train Acc: 0.9781, Val Acc: 0.9124\n",
      "Early stopping at epoch 69\n",
      "Epoch 0, Train Loss: 2.5229, Val Loss: 1.9600, Train Acc: 0.3969, Val Acc: 0.3841\n",
      "Epoch 10, Train Loss: 0.5122, Val Loss: 0.6440, Train Acc: 0.8425, Val Acc: 0.8024\n",
      "Epoch 20, Train Loss: 0.2360, Val Loss: 0.4047, Train Acc: 0.9385, Val Acc: 0.8724\n",
      "Epoch 30, Train Loss: 0.1536, Val Loss: 0.3501, Train Acc: 0.9619, Val Acc: 0.8959\n",
      "Epoch 40, Train Loss: 0.1213, Val Loss: 0.3871, Train Acc: 0.9614, Val Acc: 0.8881\n",
      "Epoch 50, Train Loss: 0.0995, Val Loss: 0.3112, Train Acc: 0.9820, Val Acc: 0.9067\n",
      "Epoch 60, Train Loss: 0.0851, Val Loss: 0.3158, Train Acc: 0.9830, Val Acc: 0.9124\n",
      "Early stopping at epoch 67\n",
      "Epoch 0, Train Loss: 2.6789, Val Loss: 2.1861, Train Acc: 0.3399, Val Acc: 0.3126\n",
      "Epoch 10, Train Loss: 0.5050, Val Loss: 0.5751, Train Acc: 0.8608, Val Acc: 0.8180\n",
      "Epoch 20, Train Loss: 0.2618, Val Loss: 0.3687, Train Acc: 0.9329, Val Acc: 0.8781\n",
      "Epoch 30, Train Loss: 0.1749, Val Loss: 0.3187, Train Acc: 0.9613, Val Acc: 0.8969\n",
      "Epoch 40, Train Loss: 0.1386, Val Loss: 0.2894, Train Acc: 0.9731, Val Acc: 0.9056\n",
      "Epoch 50, Train Loss: 0.1039, Val Loss: 0.3312, Train Acc: 0.9650, Val Acc: 0.8984\n",
      "Epoch 60, Train Loss: 0.0848, Val Loss: 0.2734, Train Acc: 0.9807, Val Acc: 0.9157\n",
      "Early stopping at epoch 63\n",
      "Epoch 0, Train Loss: 2.6243, Val Loss: 2.0733, Train Acc: 0.3547, Val Acc: 0.3378\n",
      "Epoch 10, Train Loss: 0.5343, Val Loss: 0.5693, Train Acc: 0.8566, Val Acc: 0.8211\n",
      "Epoch 20, Train Loss: 0.2690, Val Loss: 0.3899, Train Acc: 0.9234, Val Acc: 0.8721\n",
      "Epoch 30, Train Loss: 0.1788, Val Loss: 0.3114, Train Acc: 0.9565, Val Acc: 0.9031\n",
      "Epoch 40, Train Loss: 0.1271, Val Loss: 0.2867, Train Acc: 0.9719, Val Acc: 0.9120\n",
      "Epoch 50, Train Loss: 0.1035, Val Loss: 0.3152, Train Acc: 0.9624, Val Acc: 0.9040\n",
      "Epoch 60, Train Loss: 0.1008, Val Loss: 0.2811, Train Acc: 0.9708, Val Acc: 0.9118\n",
      "Epoch 70, Train Loss: 0.0734, Val Loss: 0.2828, Train Acc: 0.9795, Val Acc: 0.9122\n",
      "Epoch 80, Train Loss: 0.0831, Val Loss: 0.2515, Train Acc: 0.9844, Val Acc: 0.9200\n",
      "Early stopping at epoch 86\n",
      "Epoch 0, Train Loss: 2.6962, Val Loss: 2.1765, Train Acc: 0.3225, Val Acc: 0.3233\n",
      "Epoch 10, Train Loss: 0.5130, Val Loss: 0.5859, Train Acc: 0.8547, Val Acc: 0.8119\n",
      "Epoch 20, Train Loss: 0.2511, Val Loss: 0.4023, Train Acc: 0.9239, Val Acc: 0.8749\n",
      "Epoch 30, Train Loss: 0.1587, Val Loss: 0.3134, Train Acc: 0.9653, Val Acc: 0.9010\n",
      "Epoch 40, Train Loss: 0.1261, Val Loss: 0.3039, Train Acc: 0.9730, Val Acc: 0.9042\n",
      "Epoch 50, Train Loss: 0.0906, Val Loss: 0.3174, Train Acc: 0.9732, Val Acc: 0.9037\n",
      "Epoch 60, Train Loss: 0.0784, Val Loss: 0.3139, Train Acc: 0.9732, Val Acc: 0.9072\n",
      "Epoch 70, Train Loss: 0.0719, Val Loss: 0.3229, Train Acc: 0.9752, Val Acc: 0.9029\n",
      "Epoch 80, Train Loss: 0.0676, Val Loss: 0.2536, Train Acc: 0.9888, Val Acc: 0.9212\n",
      "Epoch 90, Train Loss: 0.0616, Val Loss: 0.2721, Train Acc: 0.9885, Val Acc: 0.9198\n",
      "Early stopping at epoch 98\n",
      "Epoch 0, Train Loss: 2.4967, Val Loss: 1.9143, Train Acc: 0.4058, Val Acc: 0.4053\n",
      "Epoch 10, Train Loss: 0.5481, Val Loss: 0.5881, Train Acc: 0.8437, Val Acc: 0.8121\n",
      "Epoch 20, Train Loss: 0.2770, Val Loss: 0.3742, Train Acc: 0.9315, Val Acc: 0.8747\n",
      "Epoch 30, Train Loss: 0.1730, Val Loss: 0.3268, Train Acc: 0.9545, Val Acc: 0.8950\n",
      "Epoch 40, Train Loss: 0.1298, Val Loss: 0.2943, Train Acc: 0.9716, Val Acc: 0.9054\n",
      "Epoch 50, Train Loss: 0.1010, Val Loss: 0.2993, Train Acc: 0.9698, Val Acc: 0.9084\n",
      "Early stopping at epoch 59\n",
      "Cross-validation accuracies: [0.905268759978712, 0.9132517296434274, 0.9111071682044003, 0.9208658623136976, 0.9118168914123492]\n",
      "Mean CV accuracy: 0.9125 Â± 0.0050\n",
      "\n",
      "Classification Report (Test set):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          ya       0.98      0.99      0.98       854\n",
      "         yab       0.92      0.98      0.95       854\n",
      "        yach       0.97      0.98      0.98       854\n",
      "         yad       0.97      0.99      0.98       854\n",
      "        yadd       0.85      0.99      0.91       854\n",
      "         yae       0.99      0.97      0.98       854\n",
      "         yaf       0.97      0.99      0.98       854\n",
      "         yag       0.98      0.98      0.98       854\n",
      "        yagg       0.97      0.99      0.98       854\n",
      "        yagh       0.99      0.98      0.99       854\n",
      "         yah       0.99      0.95      0.97       854\n",
      "        yahh       0.97      1.00      0.98       854\n",
      "         yaj       0.97      0.97      0.97       854\n",
      "         yak       0.97      0.97      0.97       854\n",
      "        yakk       0.98      0.97      0.98       854\n",
      "         yal       0.99      0.94      0.97       854\n",
      "         yam       0.98      0.99      0.98       854\n",
      "         yan       0.97      0.99      0.98       854\n",
      "         yaq       0.99      0.97      0.98       854\n",
      "         yar       0.98      0.87      0.92       854\n",
      "        yarr       0.97      0.96      0.97       854\n",
      "         yas       0.94      0.94      0.94       854\n",
      "        yass       0.94      0.98      0.96       854\n",
      "         yat       1.00      0.98      0.99       854\n",
      "        yatt       0.96      0.85      0.90       854\n",
      "         yaw       1.00      0.97      0.98       854\n",
      "         yax       0.99      0.98      0.98       854\n",
      "         yay       0.97      0.98      0.98       854\n",
      "         yaz       0.85      1.00      0.92       854\n",
      "        yazz       0.99      0.83      0.90       854\n",
      "         yey       0.98      0.95      0.97       854\n",
      "          yi       0.97      0.97      0.97       854\n",
      "          yu       0.94      0.98      0.96       854\n",
      "\n",
      "    accuracy                           0.96     28182\n",
      "   macro avg       0.97      0.96      0.96     28182\n",
      "weighted avg       0.97      0.96      0.96     28182\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    \"\"\"\n",
    "    ReLU activation: max(0, x)\n",
    "    \"\"\"\n",
    "    assert isinstance(x, np.ndarray), \"Input to ReLU must be a numpy array\"\n",
    "    result = np.maximum(0, x)\n",
    "    assert np.all(result >= 0), \"ReLU output must be non-negative\"\n",
    "    return result\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"\n",
    "    Derivative of ReLU: 1 if x > 0, else 0\n",
    "    \"\"\"\n",
    "    assert isinstance(x, np.ndarray), \"Input to ReLU derivative must be a numpy array\"\n",
    "    result = np.where(x > 0, 1, 0)\n",
    "    assert np.all((result == 0) | (result == 1)), \"ReLU derivative must be 0 or 1\"\n",
    "    return result\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"\n",
    "    Softmax activation: exp(x) / sum(exp(x))\n",
    "    \"\"\"\n",
    "    assert isinstance(x, np.ndarray), \"Input to softmax must be a numpy array\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))  # Numerical stability\n",
    "    result = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "    assert np.all((result >= 0) & (result <= 1)), \"Softmax output must be in [0, 1]\"\n",
    "    assert np.allclose(np.sum(result, axis=1), 1), \"Softmax output must sum to 1 per sample\"\n",
    "    return result\n",
    "\n",
    "# Neural Network class\n",
    "class MultiClassNeuralNetwork:\n",
    "    def __init__(self, layer_sizes, learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        \"\"\"\n",
    "        Initialize the neural network with given layer sizes and learning rate.\n",
    "        layer_sizes: List of integers [input_size, hidden1_size, ..., output_size]\n",
    "        Uses Adam optimizer parameters: beta1, beta2, epsilon\n",
    "        \"\"\"\n",
    "        assert isinstance(layer_sizes, list) and len(layer_sizes) >= 2, \"layer_sizes must be a list with at least 2 elements\"\n",
    "        assert all(isinstance(size, int) and size > 0 for size in layer_sizes), \"All layer sizes must be positive integers\"\n",
    "        assert isinstance(learning_rate, (int, float)) and learning_rate > 0, \"Learning rate must be a positive number\"\n",
    "        \n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        np.random.seed(42)\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01\n",
    "            b = np.zeros((1, layer_sizes[i+1]))\n",
    "            assert w.shape == (layer_sizes[i], layer_sizes[i+1]), f\"Weight matrix {i+1} has incorrect shape\"\n",
    "            assert b.shape == (1, layer_sizes[i+1]), f\"Bias vector {i+1} has incorrect shape\"\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "        \n",
    "        # Initialize Adam moment estimates\n",
    "        self.m_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        self.v_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        self.m_biases = [np.zeros_like(b) for b in self.biases]\n",
    "        self.v_biases = [np.zeros_like(b) for b in self.biases]\n",
    "        self.t = 0  # Time step for Adam\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation: Z^{[l]} = A^{[l-1]} W^{[l]} + b^{[l]}, A^{[l]} = g(Z^{[l]})\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray), \"Input X must be a numpy array\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        \n",
    "        self.activations = [X]\n",
    "        self.z_values = []\n",
    "        \n",
    "        for i in range(len(self.weights) - 1):\n",
    "            z = self.activations[-1] @ self.weights[i] + self.biases[i]\n",
    "            assert z.shape == (X.shape[0], self.layer_sizes[i+1]), f\"Z^{[i+1]} has incorrect shape\"\n",
    "            self.z_values.append(z)\n",
    "            self.activations.append(relu(z))\n",
    "        \n",
    "        z = self.activations[-1] @ self.weights[-1] + self.biases[-1]\n",
    "        assert z.shape == (X.shape[0], self.layer_sizes[-1]), \"Output Z has incorrect shape\"\n",
    "        self.z_values.append(z)\n",
    "        output = softmax(z)\n",
    "        assert output.shape == (X.shape[0], self.layer_sizes[-1]), \"Output A has incorrect shape\"\n",
    "        self.activations.append(output)\n",
    "        \n",
    "        return self.activations[-1]\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Categorical Cross-Entropy: J = -1/m * sum(y_true * log(y_pred))\n",
    "        \"\"\"\n",
    "        assert isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray), \"Inputs to loss must be numpy arrays\"\n",
    "        assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n",
    "        \n",
    "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "        loss = -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "        assert not np.isnan(loss), \"Loss computation resulted in NaN\"\n",
    "        return loss\n",
    "\n",
    "    def compute_accuracy(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute accuracy: proportion of correct predictions\n",
    "        \"\"\"\n",
    "        assert isinstance(y_true, np.ndarray) and isinstance(y_pred, np.ndarray), \"Inputs to accuracy must be numpy arrays\"\n",
    "        assert y_true.shape == y_pred.shape, \"y_true and y_pred must have the same shape\"\n",
    "        \n",
    "        predictions = np.argmax(y_pred, axis=1)\n",
    "        true_labels = np.argmax(y_true, axis=1)\n",
    "        accuracy = np.mean(predictions == true_labels)\n",
    "        assert 0 <= accuracy <= 1, \"Accuracy must be between 0 and 1\"\n",
    "        return accuracy\n",
    "\n",
    "    def backward(self, X, y, outputs):\n",
    "        \"\"\"\n",
    "        Backpropagation: compute dW^{[l]}, db^{[l]} for each layer with Adam optimizer\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray) and isinstance(y, np.ndarray) and isinstance(outputs, np.ndarray), \"Inputs to backward must be numpy arrays\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        assert y.shape == outputs.shape, \"y and outputs must have the same shape\"\n",
    "        \n",
    "        m = X.shape[0]\n",
    "        self.d_weights = [np.zeros_like(w) for w in self.weights]\n",
    "        self.d_biases = [np.zeros_like(b) for b in self.biases]\n",
    "        \n",
    "        dZ = outputs - y  # Gradient for softmax + cross-entropy\n",
    "        assert dZ.shape == outputs.shape, \"dZ for output layer has incorrect shape\"\n",
    "        self.d_weights[-1] = (self.activations[-2].T @ dZ) / m\n",
    "        self.d_biases[-1] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        \n",
    "        for i in range(len(self.weights) - 2, -1, -1):\n",
    "            dZ = (dZ @ self.weights[i+1].T) * relu_derivative(self.z_values[i])\n",
    "            assert dZ.shape == (X.shape[0], self.layer_sizes[i+1]), f\"dZ^{[i+1]} has incorrect shape\"\n",
    "            self.d_weights[i] = (self.activations[i].T @ dZ) / m\n",
    "            self.d_biases[i] = np.sum(dZ, axis=0, keepdims=True) / m\n",
    "        \n",
    "        # L2 regularization\n",
    "        l2_lambda = 0.01\n",
    "        for i in range(len(self.weights)):\n",
    "            self.d_weights[i] += l2_lambda * self.weights[i] / m\n",
    "        \n",
    "        # Adam optimizer updates\n",
    "        self.t += 1\n",
    "        for i in range(len(self.weights)):\n",
    "            # Update biased first moment estimate\n",
    "            self.m_weights[i] = self.beta1 * self.m_weights[i] + (1 - self.beta1) * self.d_weights[i]\n",
    "            self.m_biases[i] = self.beta1 * self.m_biases[i] + (1 - self.beta1) * self.d_biases[i]\n",
    "            # Update biased second moment estimate\n",
    "            self.v_weights[i] = self.beta2 * self.v_weights[i] + (1 - self.beta2) * (self.d_weights[i] ** 2)\n",
    "            self.v_biases[i] = self.beta2 * self.v_biases[i] + (1 - self.beta2) * (self.d_biases[i] ** 2)\n",
    "            # Bias-corrected moments\n",
    "            m_hat_w = self.m_weights[i] / (1 - self.beta1 ** self.t)\n",
    "            m_hat_b = self.m_biases[i] / (1 - self.beta1 ** self.t)\n",
    "            v_hat_w = self.v_weights[i] / (1 - self.beta2 ** self.t)\n",
    "            v_hat_b = self.v_biases[i] / (1 - self.beta2 ** self.t)\n",
    "            # Update weights and biases\n",
    "            self.weights[i] -= self.learning_rate * m_hat_w / (np.sqrt(v_hat_w) + self.epsilon)\n",
    "            self.biases[i] -= self.learning_rate * m_hat_b / (np.sqrt(v_hat_b) + self.epsilon)\n",
    "\n",
    "    def train(self, X, y, X_val, y_val, epochs, batch_size, patience=10):\n",
    "        \"\"\"\n",
    "        Train the neural network using mini-batch SGD with validation and early stopping\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray) and isinstance(y, np.ndarray), \"X and y must be numpy arrays\"\n",
    "        assert isinstance(X_val, np.ndarray) and isinstance(y_val, np.ndarray), \"X_val and y_val must be numpy arrays\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        assert y.shape[1] == self.layer_sizes[-1], f\"Output dimension ({y.shape[1]}) must match output layer size ({self.layer_sizes[-1]})\"\n",
    "        assert X_val.shape[1] == self.layer_sizes[0], f\"Validation input dimension ({X_val.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        assert y_val.shape[1] == self.layer_sizes[-1], f\"Validation output dimension ({y_val.shape[1]}) must match output layer size ({self.layer_sizes[-1]})\"\n",
    "        assert isinstance(epochs, int) and epochs > 0, \"Epochs must be a positive integer\"\n",
    "        assert isinstance(batch_size, int) and batch_size > 0, \"Batch size must be a positive integer\"\n",
    "        \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        train_accuracies = []\n",
    "        val_accuracies = []\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            indices = np.random.permutation(X.shape[0])\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "            epoch_loss = 0\n",
    "            for i in range(0, X.shape[0], batch_size):\n",
    "                X_batch = X_shuffled[i:i+batch_size]\n",
    "                y_batch = y_shuffled[i:i+batch_size]\n",
    "                outputs = self.forward(X_batch)\n",
    "                epoch_loss += self.compute_loss(y_batch, outputs)\n",
    "                self.backward(X_batch, y_batch, outputs)\n",
    "            \n",
    "            train_loss = epoch_loss / (X.shape[0] // batch_size)\n",
    "            train_pred = self.forward(X)\n",
    "            train_accuracy = self.compute_accuracy(y, train_pred)\n",
    "            val_pred = self.forward(X_val)\n",
    "            val_loss = self.compute_loss(y_val, val_pred)\n",
    "            val_accuracy = self.compute_accuracy(y_val, val_pred)\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            train_accuracies.append(train_accuracy)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
    "                      f\"Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch}\")\n",
    "                break\n",
    "        \n",
    "        return train_losses, val_losses, train_accuracies, val_accuracies\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict class labels\n",
    "        \"\"\"\n",
    "        assert isinstance(X, np.ndarray), \"Input X must be a numpy array\"\n",
    "        assert X.shape[1] == self.layer_sizes[0], f\"Input dimension ({X.shape[1]}) must match input layer size ({self.layer_sizes[0]})\"\n",
    "        \n",
    "        outputs = self.forward(X)\n",
    "        predictions = np.argmax(outputs, axis=1)\n",
    "        assert predictions.shape == (X.shape[0],), \"Predictions have incorrect shape\"\n",
    "        return predictions\n",
    "\n",
    "# Cross-validation function\n",
    "def cross_validate(X, y, layer_sizes, learning_rate, epochs, batch_size, k=5):\n",
    "    \"\"\"\n",
    "    Perform k-fold cross-validation\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "    val_accuracies = []\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        y_train_one_hot = one_hot_encoder.fit_transform(y_train.reshape(-1, 1))\n",
    "        y_val_one_hot = one_hot_encoder.transform(y_val.reshape(-1, 1))\n",
    "        nn = MultiClassNeuralNetwork(layer_sizes, learning_rate)\n",
    "        _, _, _, val_accs = nn.train(X_train, y_train_one_hot, X_val, y_val_one_hot, epochs, batch_size)\n",
    "        val_accuracies.append(val_accs[-1])\n",
    "    print(f\"Cross-validation accuracies: {val_accuracies}\")\n",
    "    print(f\"Mean CV accuracy: {np.mean(val_accuracies):.4f} Â± {np.std(val_accuracies):.4f}\")\n",
    "    return val_accuracies\n",
    "\n",
    "# Define paths to dataset\n",
    "train_dir = r\"C:\\Users\\Dell\\Desktop\\Tifinagh\\train_data\"\n",
    "test_dir = r\"C:\\Users\\Dell\\Desktop\\Tifinagh\\test_data\"\n",
    "print(f\"Train directory: {train_dir}\")\n",
    "print(f\"Test directory: {test_dir}\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Verify directories exist\n",
    "if not os.path.exists(train_dir):\n",
    "    raise FileNotFoundError(f\"Train directory not found: {train_dir}\")\n",
    "if not os.path.exists(test_dir):\n",
    "    raise FileNotFoundError(f\"Test directory not found: {test_dir}\")\n",
    "\n",
    "# Build DataFrame from train_data folder structure\n",
    "train_image_paths = []\n",
    "train_labels = []\n",
    "for label_dir in os.listdir(train_dir):\n",
    "    label_path = os.path.join(train_dir, label_dir)\n",
    "    if os.path.isdir(label_path):\n",
    "        for img_name in os.listdir(label_path):\n",
    "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                train_image_paths.append(os.path.join(label_dir, img_name))  # Relative path\n",
    "                train_labels.append(label_dir)\n",
    "train_df = pd.DataFrame({'image_path': train_image_paths, 'label': train_labels})\n",
    "\n",
    "# Build DataFrame from test_data folder structure\n",
    "test_image_paths = []\n",
    "test_labels = []\n",
    "for label_dir in os.listdir(test_dir):\n",
    "    label_path = os.path.join(test_dir, label_dir)\n",
    "    if os.path.isdir(label_path):\n",
    "        for img_name in os.listdir(label_path):\n",
    "            if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                test_image_paths.append(os.path.join(label_dir, img_name))  # Relative path\n",
    "                test_labels.append(label_dir)\n",
    "test_df = pd.DataFrame({'image_path': test_image_paths, 'label': test_labels})\n",
    "\n",
    "# Verify DataFrames\n",
    "assert not train_df.empty, \"No training data loaded. Check train_data directory.\"\n",
    "assert not test_df.empty, \"No test data loaded. Check test_data directory.\"\n",
    "print(f\"Loaded {len(train_df)} training samples with {train_df['label'].nunique()} unique classes.\")\n",
    "print(f\"Loaded {len(test_df)} test samples with {test_df['label'].nunique()} unique classes.\")\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "train_df['label_encoded'] = label_encoder.fit_transform(train_df['label'])\n",
    "test_df['label_encoded'] = label_encoder.transform(test_df['label'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "assert num_classes == 33, f\"Expected 33 classes, got {num_classes}\"\n",
    "\n",
    "# Function to load and preprocess images\n",
    "def load_and_preprocess_image(image_path, base_dir, target_size=(32, 32)):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image: convert to grayscale, resize, normalize\n",
    "    \"\"\"\n",
    "    full_path = os.path.join(base_dir, image_path)\n",
    "    assert os.path.exists(full_path), f\"Image not found: {full_path}\"\n",
    "    img = cv2.imread(full_path, cv2.IMREAD_GRAYSCALE)\n",
    "    assert img is not None, f\"Failed to load image: {full_path}\"\n",
    "    img = cv2.resize(img, target_size)\n",
    "    img = img.astype(np.float32) / 255.0  # Normalize\n",
    "    return img.flatten()  # Flatten for MLP\n",
    "\n",
    "# Load training and test images\n",
    "X_train = np.array([load_and_preprocess_image(path, train_dir) for path in train_df['image_path']])\n",
    "y_train = train_df['label_encoded'].values\n",
    "X_test = np.array([load_and_preprocess_image(path, test_dir) for path in test_df['image_path']])\n",
    "y_test = test_df['label_encoded'].values\n",
    "\n",
    "# Verify dimensions\n",
    "assert X_train.shape[0] == y_train.shape[0], \"Mismatch between number of training images and labels\"\n",
    "assert X_test.shape[0] == y_test.shape[0], \"Mismatch between number of test images and labels\"\n",
    "assert X_train.shape[1] == 32 * 32, f\"Expected flattened image size of {32*32}, got {X_train.shape[1]}\"\n",
    "assert X_test.shape[1] == 32 * 32, f\"Expected flattened image size of {32*32}, got {X_test.shape[1]}\"\n",
    "\n",
    "# Split training data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, stratify=y_train, random_state=42)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X_train = np.array(X_train)\n",
    "X_val = np.array(X_val)\n",
    "X_test = np.array(X_test)\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "print(f\"Train: {X_train.shape[0]} samples, Validation: {X_val.shape[0]} samples, Test: {X_test.shape[0]} samples\")\n",
    "\n",
    "# One-hot encode labels\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_one_hot = np.array(one_hot_encoder.fit_transform(y_train.reshape(-1, 1)))\n",
    "y_val_one_hot = np.array(one_hot_encoder.transform(y_val.reshape(-1, 1)))\n",
    "y_test_one_hot = np.array(one_hot_encoder.transform(y_test.reshape(-1, 1)))\n",
    "\n",
    "# Verify one-hot arrays\n",
    "assert isinstance(y_train_one_hot, np.ndarray), \"y_train_one_hot must be a numpy array\"\n",
    "assert isinstance(y_val_one_hot, np.ndarray), \"y_val_one_hot must be a numpy array\"\n",
    "assert isinstance(y_test_one_hot, np.ndarray), \"y_test_one_hot must be a numpy array\"\n",
    "\n",
    "# Create and train the model\n",
    "layer_sizes = [X_train.shape[1], 64, 32, num_classes]  # [1024, 64, 32, 33]\n",
    "nn = MultiClassNeuralNetwork(layer_sizes, learning_rate=0.001)\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = nn.train(\n",
    "    X_train, y_train_one_hot, X_val, y_val_one_hot, epochs=100, batch_size=32\n",
    ")\n",
    "\n",
    "# Cross-validation\n",
    "X_temp = np.concatenate((X_train, X_val), axis=0)\n",
    "y_temp = np.concatenate((y_train, y_val), axis=0)\n",
    "cv_accuracies = cross_validate(X_temp, y_temp, layer_sizes, learning_rate=0.001, epochs=100, batch_size=32, k=5)\n",
    "\n",
    "# Predictions and evaluation on test set\n",
    "y_pred = nn.predict(X_test)\n",
    "print(\"\\nClassification Report (Test set):\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix (Test set)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# Loss and accuracy curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Loss curve\n",
    "ax1.plot(train_losses, label='Train Loss')\n",
    "ax1.plot(val_losses, label='Validation Loss')\n",
    "ax1.set_title('Loss Curve')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.legend()\n",
    "\n",
    "# Accuracy curve\n",
    "ax2.plot(train_accuracies, label='Train Accuracy')\n",
    "ax2.plot(val_accuracies, label='Validation Accuracy')\n",
    "ax2.set_title('Accuracy Curve')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('loss_accuracy_plot.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efc549a-8606-4382-9f75-fa18d5f52ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
